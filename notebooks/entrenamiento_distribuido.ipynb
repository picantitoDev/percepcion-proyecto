{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba11a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.torch.distributor import TorchDistributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f55e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base = models.resnet18(weights=None)\n",
    "        in_features = base.fc.in_features\n",
    "\n",
    "        base.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "        # Se deben copiar las capas igual que en train_fn\n",
    "        self.conv1 = base.conv1\n",
    "        self.bn1 = base.bn1\n",
    "        self.relu = base.relu\n",
    "        self.maxpool = base.maxpool\n",
    "\n",
    "        self.layer1 = base.layer1\n",
    "        self.layer2 = base.layer2\n",
    "        self.layer3 = base.layer3\n",
    "        self.layer4 = base.layer4\n",
    "\n",
    "        self.avgpool = base.avgpool\n",
    "        self.fc = base.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6cf84d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 03:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sesi√≥n de Spark creada exitosamente\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"BrainTumor-ResNet18-Distributed-HDFS\")\n",
    "        .master(\"spark://100.108.67.1:7077\")\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "\n",
    "        # GPU CONFIG\n",
    "        .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "        .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "        .config(\"spark.executor.resource.gpu.discoveryScript\", \"/usr/local/bin/get-gpus.sh\")\n",
    "\n",
    "        # Networking\n",
    "        .config(\"spark.executorEnv.NCCL_SOCKET_IFNAME\", \"tailscale0\")\n",
    "        .config(\"spark.executorEnv.GLOO_SOCKET_IFNAME\", \"tailscale0\")\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úì Sesi√≥n de Spark creada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ef0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    epochs=3,\n",
    "    lr=1e-4,\n",
    "    batch_size=32,\n",
    "    optimizer_name=\"adam\"\n",
    "):\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import io\n",
    "    import os\n",
    "    import pyarrow.fs as fs\n",
    "    from torchvision import transforms, models\n",
    "    from torch.utils.data import DataLoader, ConcatDataset\n",
    "    from PIL import Image\n",
    "\n",
    "    print(\"=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\", flush=True)\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "    # Shards balanceados\n",
    "    train_shards = [\n",
    "        f\"hdfs://namenode:9000/data/brain_balanced/shard_bal_{i:04d}.pt\"\n",
    "        for i in range(8)\n",
    "    ]\n",
    "\n",
    "    if rank == 0:\n",
    "        my_shards = train_shards[0:8:2]\n",
    "    else:\n",
    "        my_shards = train_shards[1:8:2]\n",
    "\n",
    "    class ShardDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, path, transform):\n",
    "            fs_conn = fs.HadoopFileSystem(\"namenode\", 9000)\n",
    "            with fs_conn.open_input_file(path) as f:\n",
    "                data = f.read()\n",
    "            shard = torch.load(io.BytesIO(data))\n",
    "            self.images = shard[\"images\"]\n",
    "            self.labels = shard[\"labels\"].long()\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, i):\n",
    "            img = Image.fromarray(self.images[i].numpy())\n",
    "            return self.transform(img), self.labels[i]\n",
    "\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    datasets = [ShardDataset(s, tf) for s in my_shards]\n",
    "    full = ConcatDataset(datasets)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        full,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # =======================\n",
    "    # MODELO CON 7 CAPAS EXTRA\n",
    "    # =======================\n",
    "    base = models.resnet18(weights=None)\n",
    "\n",
    "    in_features = base.fc.in_features\n",
    "\n",
    "    # Reemplazamos 'fc' por un head profundo\n",
    "    base.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(256),\n",
    "\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(128),\n",
    "\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(32, 2) # SALIDA FINAL\n",
    "    )\n",
    "\n",
    "    model = base.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # OPTIMIZADOR DIN√ÅMICO\n",
    "    opt_name = optimizer_name.lower()\n",
    "    if opt_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizador no soportado: {optimizer_name}\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    # ENTRENAMIENTO\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"[Worker {rank}] √âpoca {epoch+1}/{epochs} P√©rdida={total_loss/steps:.4f}\")\n",
    "\n",
    "    # DEVOLVER MODELO SOLO EN RANK 0\n",
    "    if rank == 0:\n",
    "        buf = io.BytesIO()\n",
    "        torch.save(model.state_dict(), buf)\n",
    "        buf.seek(0)\n",
    "        return buf.getvalue()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6520c73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as picantitoDev\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as picantitoDev\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"picantitoDev/percepcion-proyecto\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"picantitoDev/percepcion-proyecto\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository picantitoDev/percepcion-proyecto initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository picantitoDev/percepcion-proyecto initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/7f8fca52276a46fd914d2197509806ad', creation_time=1764140493088, experiment_id='1', last_update_time=1764140493088, lifecycle_stage='active', name='Proyecto Percepcion', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CONFIGURACI√ìN DE EXPERIMENTOS M√öLTIPLES\n",
    "# ====================================================================\n",
    "\n",
    "import mlflow\n",
    "import dagshub\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Inicializar Dagshub + MLflow (UNA SOLA VEZ al inicio)\n",
    "dagshub.init(\n",
    "    repo_owner='picantitoDev',\n",
    "    repo_name='percepcion-proyecto',\n",
    "    mlflow=True\n",
    ")\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow\")\n",
    "mlflow.set_experiment(\"Proyecto Percepcion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed4954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los experimentos\n",
    "experiments = [\n",
    "    {\"epochs\": 5, \"lr\": 0.01, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    {\"epochs\": 5, \"lr\": 0.001, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    {\"epochs\": 5, \"lr\": 0.0001, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    \n",
    "    {\"epochs\": 10, \"lr\": 0.01, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    {\"epochs\": 10, \"lr\": 0.001, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    {\"epochs\": 10, \"lr\": 0.0001, \"batch_size\": 32, \"optimizer\": \"adam\"},\n",
    "    \n",
    "    {\"epochs\": 15, \"lr\": 0.01, \"batch_size\": 32, \"optimizer\": \"sgd\"},\n",
    "    {\"epochs\": 15, \"lr\": 0.001, \"batch_size\": 32, \"optimizer\": \"sgd\"},\n",
    "    {\"epochs\": 15, \"lr\": 0.0001, \"batch_size\": 32, \"optimizer\": \"sgd\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a927848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test shards (constante para todos los experimentos)\n",
    "test_shards = [\n",
    "    \"hdfs://namenode:9000/data/brain_balanced/shard_bal_0008.pt\",\n",
    "    \"hdfs://namenode:9000/data/brain_balanced/shard_bal_0009.pt\"\n",
    "]\n",
    "\n",
    "# Transformaciones para test (constante)\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485,0.456,0.406],\n",
    "        std=[0.229,0.224,0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def load_pt_with_spark(hdfs_path):\n",
    "    content = spark.sparkContext.binaryFiles(hdfs_path).take(1)[0][1]\n",
    "    return torch.load(io.BytesIO(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84178cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 1/9\n",
      " Config: {'epochs': 5, 'lr': 0.01, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:32:51,935 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:32:51,521 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/5 P√©rdida=0.6239\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 2/5 P√©rdida=0.5846\n",
      "[Worker 1] √âpoca 3/5 P√©rdida=0.5556\n",
      "[Worker 0] √âpoca 1/5 P√©rdida=0.5934\n",
      "[Worker 1] √âpoca 4/5 P√©rdida=0.5247\n",
      "[Worker 0] √âpoca 2/5 P√©rdida=0.5483\n",
      "[Worker 1] √âpoca 5/5 P√©rdida=0.5619\n",
      "[Worker 0] √âpoca 3/5 P√©rdida=0.5191\n",
      "[Worker 0] √âpoca 4/5 P√©rdida=0.4753\n",
      "[Worker 0] √âpoca 5/5 P√©rdida=0.4480\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 1:\n",
      "  Loss:      0.4216\n",
      "  Accuracy:  0.7990\n",
      "  Precision: 0.7447\n",
      "  Recall:    0.9100\n",
      "  F1:        0.8191\n",
      "üèÉ View run exp_1_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/7dd3202c9cea4b75b6d85c7bf927d69a\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.8191\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 2/9\n",
      " Config: {'epochs': 5, 'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:34:54,762 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:34:58,024 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 1/5 P√©rdida=0.5838\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 2/5 P√©rdida=0.4558\n",
      "[Worker 1] √âpoca 1/5 P√©rdida=0.5917\n",
      "[Worker 0] √âpoca 3/5 P√©rdida=0.3955\n",
      "[Worker 1] √âpoca 2/5 P√©rdida=0.4597\n",
      "[Worker 0] √âpoca 4/5 P√©rdida=0.2941\n",
      "[Worker 1] √âpoca 3/5 P√©rdida=0.3773\n",
      "[Worker 1] √âpoca 4/5 P√©rdida=0.2879\n",
      "[Worker 0] √âpoca 5/5 P√©rdida=0.3140\n",
      "[Worker 1] √âpoca 5/5 P√©rdida=0.2725                                 (0 + 2) / 2]\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 2:\n",
      "  Loss:      0.4318\n",
      "  Accuracy:  0.8700\n",
      "  Precision: 0.9302\n",
      "  Recall:    0.8000\n",
      "  F1:        0.8602\n",
      "üèÉ View run exp_2_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/6407a49996264a46877459157c467eda\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.8602\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 3/9\n",
      " Config: {'epochs': 5, 'lr': 0.0001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:37:15,651 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:37:18,740 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/5 P√©rdida=0.6585\n",
      "[Worker 1] √âpoca 2/5 P√©rdida=0.4574\n",
      "[Worker 1] √âpoca 3/5 P√©rdida=0.2497\n",
      "[Worker 1] √âpoca 4/5 P√©rdida=0.0949\n",
      "[Worker 1] √âpoca 5/5 P√©rdida=0.0955\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 1/5 P√©rdida=0.6658\n",
      "[Worker 0] √âpoca 2/5 P√©rdida=0.5051\n",
      "[Worker 0] √âpoca 3/5 P√©rdida=0.3030\n",
      "[Worker 0] √âpoca 4/5 P√©rdida=0.1625\n",
      "[Worker 0] √âpoca 5/5 P√©rdida=0.1118\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 3:\n",
      "  Loss:      0.4177\n",
      "  Accuracy:  0.8550\n",
      "  Precision: 0.7877\n",
      "  Recall:    0.9720\n",
      "  F1:        0.8702\n",
      "üèÉ View run exp_3_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/1d387edae41e40e1aa8b5a00f7656bfe\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.8702\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 4/9\n",
      " Config: {'epochs': 10, 'lr': 0.01, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:39:57,076 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:39:58,213 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 1/10 P√©rdida=0.6271\n",
      "[Worker 0] √âpoca 2/10 P√©rdida=0.5997\n",
      "[Worker 0] √âpoca 3/10 P√©rdida=0.6381\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 4/10 P√©rdida=0.5745\n",
      "[Worker 1] √âpoca 1/10 P√©rdida=0.6470\n",
      "[Worker 0] √âpoca 5/10 P√©rdida=0.5941\n",
      "[Worker 1] √âpoca 2/10 P√©rdida=0.5514\n",
      "[Worker 0] √âpoca 6/10 P√©rdida=0.5106\n",
      "[Worker 1] √âpoca 3/10 P√©rdida=0.5630                                (0 + 2) / 2]\n",
      "[Worker 0] √âpoca 7/10 P√©rdida=0.4832\n",
      "[Worker 0] √âpoca 8/10 P√©rdida=0.4538\n",
      "[Worker 1] √âpoca 4/10 P√©rdida=0.5268\n",
      "[Worker 0] √âpoca 9/10 P√©rdida=0.4442\n",
      "[Worker 1] √âpoca 5/10 P√©rdida=0.4681\n",
      "[Worker 0] √âpoca 10/10 P√©rdida=0.4165\n",
      "[Worker 1] √âpoca 6/10 P√©rdida=0.4034\n",
      "[Worker 1] √âpoca 7/10 P√©rdida=0.3812\n",
      "[Worker 1] √âpoca 8/10 P√©rdida=0.3338\n",
      "[Worker 1] √âpoca 9/10 P√©rdida=0.3045\n",
      "[Worker 1] √âpoca 10/10 P√©rdida=0.2458\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 4:\n",
      "  Loss:      0.6448\n",
      "  Accuracy:  0.6710\n",
      "  Precision: 0.6535\n",
      "  Recall:    0.7280\n",
      "  F1:        0.6887\n",
      "üèÉ View run exp_4_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/75bf2cdd6cd2420ebafb83e8d7c6c776\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 5/9\n",
      " Config: {'epochs': 10, 'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:42:33,769 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:42:36,385 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 1/10 P√©rdida=0.5370\n",
      "[Worker 0] √âpoca 2/10 P√©rdida=0.4148\n",
      "[Worker 0] √âpoca 3/10 P√©rdida=0.3081\n",
      "[Worker 0] √âpoca 4/10 P√©rdida=0.2340\n",
      "[Worker 0] √âpoca 5/10 P√©rdida=0.2737\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 6/10 P√©rdida=0.1834\n",
      "[Worker 1] √âpoca 1/10 P√©rdida=0.6050\n",
      "[Worker 0] √âpoca 7/10 P√©rdida=0.1520\n",
      "[Worker 1] √âpoca 2/10 P√©rdida=0.4964\n",
      "[Worker 0] √âpoca 8/10 P√©rdida=0.1399\n",
      "[Worker 0] √âpoca 9/10 P√©rdida=0.1341                                (0 + 2) / 2]\n",
      "[Worker 1] √âpoca 3/10 P√©rdida=0.4126\n",
      "[Worker 0] √âpoca 10/10 P√©rdida=0.1178\n",
      "[Worker 1] √âpoca 4/10 P√©rdida=0.3596\n",
      "[Worker 1] √âpoca 5/10 P√©rdida=0.2926\n",
      "[Worker 1] √âpoca 6/10 P√©rdida=0.2582\n",
      "[Worker 1] √âpoca 7/10 P√©rdida=0.2260\n",
      "[Worker 1] √âpoca 8/10 P√©rdida=0.1942\n",
      "[Worker 1] √âpoca 9/10 P√©rdida=0.1732\n",
      "[Worker 1] √âpoca 10/10 P√©rdida=0.1748\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 5:\n",
      "  Loss:      0.1636\n",
      "  Accuracy:  0.9440\n",
      "  Precision: 0.9933\n",
      "  Recall:    0.8940\n",
      "  F1:        0.9411\n",
      "üèÉ View run exp_5_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/3daae63c2650486cb9a885c78b58b563\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.9411\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 6/9\n",
      " Config: {'epochs': 10, 'lr': 0.0001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:45:02,488 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:45:05,701 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/10 P√©rdida=0.6648\n",
      "[Worker 1] √âpoca 2/10 P√©rdida=0.4780\n",
      "[Worker 1] √âpoca 3/10 P√©rdida=0.2525\n",
      "[Worker 1] √âpoca 4/10 P√©rdida=0.1320\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 5/10 P√©rdida=0.1034\n",
      "[Worker 0] √âpoca 1/10 P√©rdida=0.6536\n",
      "[Worker 1] √âpoca 6/10 P√©rdida=0.0667\n",
      "[Worker 0] √âpoca 2/10 P√©rdida=0.4724\n",
      "[Worker 1] √âpoca 7/10 P√©rdida=0.0765\n",
      "[Worker 0] √âpoca 3/10 P√©rdida=0.2571                                (0 + 2) / 2]\n",
      "[Worker 1] √âpoca 8/10 P√©rdida=0.0509\n",
      "[Worker 1] √âpoca 9/10 P√©rdida=0.0472\n",
      "[Worker 0] √âpoca 4/10 P√©rdida=0.1587\n",
      "[Worker 1] √âpoca 10/10 P√©rdida=0.0600\n",
      "[Worker 0] √âpoca 5/10 P√©rdida=0.1048\n",
      "[Worker 0] √âpoca 6/10 P√©rdida=0.1159\n",
      "[Worker 0] √âpoca 7/10 P√©rdida=0.0710\n",
      "[Worker 0] √âpoca 8/10 P√©rdida=0.0513\n",
      "[Worker 0] √âpoca 9/10 P√©rdida=0.0471\n",
      "[Worker 0] √âpoca 10/10 P√©rdida=0.0599\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 6:\n",
      "  Loss:      0.1402\n",
      "  Accuracy:  0.9480\n",
      "  Precision: 0.9516\n",
      "  Recall:    0.9440\n",
      "  F1:        0.9478\n",
      "üèÉ View run exp_6_adam at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/b41289702dde45999d7d7f7c4e2187c8\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.9478\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 7/9\n",
      " Config: {'epochs': 15, 'lr': 0.01, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:47:38,171 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:47:38,710 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/15 P√©rdida=0.6621\n",
      "[Worker 1] √âpoca 2/15 P√©rdida=0.4339\n",
      "[Worker 1] √âpoca 3/15 P√©rdida=0.3146\n",
      "[Worker 1] √âpoca 4/15 P√©rdida=0.2073\n",
      "[Worker 1] √âpoca 5/15 P√©rdida=0.1437\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 6/15 P√©rdida=0.1315                                (0 + 2) / 2]\n",
      "[Worker 0] √âpoca 1/15 P√©rdida=0.6424\n",
      "[Worker 1] √âpoca 7/15 P√©rdida=0.1020\n",
      "[Worker 0] √âpoca 2/15 P√©rdida=0.4123\n",
      "[Worker 1] √âpoca 8/15 P√©rdida=0.0542\n",
      "[Worker 0] √âpoca 3/15 P√©rdida=0.3178\n",
      "[Worker 1] √âpoca 9/15 P√©rdida=0.0985\n",
      "[Worker 0] √âpoca 4/15 P√©rdida=0.2486\n",
      "[Worker 1] √âpoca 10/15 P√©rdida=0.0373\n",
      "[Worker 1] √âpoca 11/15 P√©rdida=0.0256\n",
      "[Worker 0] √âpoca 5/15 P√©rdida=0.1757\n",
      "[Worker 1] √âpoca 12/15 P√©rdida=0.0649\n",
      "[Worker 0] √âpoca 6/15 P√©rdida=0.1148\n",
      "[Worker 1] √âpoca 13/15 P√©rdida=0.0367\n",
      "[Worker 0] √âpoca 7/15 P√©rdida=0.1116\n",
      "[Worker 1] √âpoca 14/15 P√©rdida=0.0455\n",
      "[Worker 0] √âpoca 8/15 P√©rdida=0.1047\n",
      "[Worker 1] √âpoca 15/15 P√©rdida=0.0195\n",
      "[Worker 0] √âpoca 9/15 P√©rdida=0.0617\n",
      "[Worker 0] √âpoca 10/15 P√©rdida=0.0794\n",
      "[Worker 0] √âpoca 11/15 P√©rdida=0.0815\n",
      "[Worker 0] √âpoca 12/15 P√©rdida=0.0499\n",
      "[Worker 0] √âpoca 13/15 P√©rdida=0.0624\n",
      "[Worker 0] √âpoca 14/15 P√©rdida=0.0352\n",
      "[Worker 0] √âpoca 15/15 P√©rdida=0.0456\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 7:\n",
      "  Loss:      0.1086\n",
      "  Accuracy:  0.9600\n",
      "  Precision: 0.9389\n",
      "  Recall:    0.9840\n",
      "  F1:        0.9609\n",
      "üèÉ View run exp_7_sgd at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/a1156fb4923847f29e5955d85da65e16\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      " ‚òÖ Nuevo mejor modelo | F1=0.9609\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 8/9\n",
      " Config: {'epochs': 15, 'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:51:38,091 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:51:39,719 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:52:40,527 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:52:40,529 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0001.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:52:40,669 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034\n",
      "2025-11-26 03:52:45,781 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:52:45,784 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0002.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:52:45,940 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035\n",
      "2025-11-26 03:53:45,836 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:53:45,837 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0003.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:53:45,921 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036\n",
      "2025-11-26 03:53:51,581 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:53:51,582 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0004.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:53:51,671 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037\n",
      "2025-11-26 03:54:51,483 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:54:51,483 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0005.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:54:51,589 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038\n",
      "2025-11-26 03:54:56,907 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:54:56,907 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0006.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:54:57,097 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/15 P√©rdida=0.6933\n",
      "[Worker 0] √âpoca 1/15 P√©rdida=0.6876\n",
      "[Worker 1] √âpoca 2/15 P√©rdida=0.6814\n",
      "[Worker 0] √âpoca 2/15 P√©rdida=0.6686\n",
      "[Worker 0] √âpoca 3/15 P√©rdida=0.6420\n",
      "[Worker 1] √âpoca 3/15 P√©rdida=0.6674\n",
      "[Worker 0] √âpoca 4/15 P√©rdida=0.5955\n",
      "[Worker 1] √âpoca 4/15 P√©rdida=0.6427\n",
      "[Worker 0] √âpoca 5/15 P√©rdida=0.5118\n",
      "[Worker 1] √âpoca 5/15 P√©rdida=0.5938\n",
      "[Worker 0] √âpoca 6/15 P√©rdida=0.3823\n",
      "[Worker 1] √âpoca 6/15 P√©rdida=0.5078\n",
      "[Worker 0] √âpoca 7/15 P√©rdida=0.2758\n",
      "[Worker 1] √âpoca 7/15 P√©rdida=0.3797\n",
      "[Worker 0] √âpoca 8/15 P√©rdida=0.2173\n",
      "[Worker 1] √âpoca 8/15 P√©rdida=0.2767\n",
      "[Worker 0] √âpoca 9/15 P√©rdida=0.2182\n",
      "[Worker 0] √âpoca 10/15 P√©rdida=0.1449\n",
      "[Worker 1] √âpoca 9/15 P√©rdida=0.2222\n",
      "[Worker 0] √âpoca 11/15 P√©rdida=0.0929                               (0 + 2) / 2]\n",
      "[Worker 1] √âpoca 10/15 P√©rdida=0.1595\n",
      "[Worker 0] √âpoca 12/15 P√©rdida=0.0602\n",
      "[Worker 1] √âpoca 11/15 P√©rdida=0.1245\n",
      "[Worker 0] √âpoca 13/15 P√©rdida=0.0645\n",
      "[Worker 1] √âpoca 12/15 P√©rdida=0.0727\n",
      "[Worker 0] √âpoca 14/15 P√©rdida=0.0781\n",
      "[Worker 1] √âpoca 13/15 P√©rdida=0.0977\n",
      "[Worker 0] √âpoca 15/15 P√©rdida=0.0602\n",
      "[Worker 1] √âpoca 14/15 P√©rdida=0.1342\n",
      "[Worker 1] √âpoca 15/15 P√©rdida=0.0629\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 8:\n",
      "  Loss:      0.2022\n",
      "  Accuracy:  0.9300\n",
      "  Precision: 0.8826\n",
      "  Recall:    0.9920\n",
      "  F1:        0.9341\n",
      "üèÉ View run exp_8_sgd at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/c67824f8b17e49969efda4543bc17a7f\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENTO 9/9\n",
      " Config: {'epochs': 15, 'lr': 0.0001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===                          (0 + 2) / 2]\n",
      "=== ENTRENAMIENTO DISTRIBUIDO INICIADO ===\n",
      "2025-11-26 03:56:59,260 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:57:00,224 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 03:58:01,091 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746857_6033\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:58:01,093 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0000.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746857_6033, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:58:01,286 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746857_6033\n",
      "2025-11-26 03:58:02,321 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:58:02,324 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0001.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:58:03,660 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746858_6034\n",
      "2025-11-26 03:59:08,223 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:59:08,223 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0002.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:59:10,363 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746859_6035\n",
      "2025-11-26 03:59:11,321 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:59:11,322 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0003.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 03:59:11,666 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746860_6036\n",
      "2025-11-26 04:00:16,980 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:00:16,980 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0004.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:00:17,119 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746861_6037\n",
      "2025-11-26 04:00:19,851 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:00:19,852 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0005.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:00:20,077 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746862_6038\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] √âpoca 1/15 P√©rdida=0.6950\n",
      "[Worker 1] √âpoca 2/15 P√©rdida=0.6936\n",
      "[Worker 1] √âpoca 3/15 P√©rdida=0.6925\n",
      "[Worker 1] √âpoca 4/15 P√©rdida=0.6913\n",
      "[Worker 1] √âpoca 5/15 P√©rdida=0.6907\n",
      "[Worker 1] √âpoca 6/15 P√©rdida=0.6893\n",
      "[Worker 1] √âpoca 7/15 P√©rdida=0.6881\n",
      "[Worker 1] √âpoca 8/15 P√©rdida=0.6870\n",
      "[Worker 1] √âpoca 9/15 P√©rdida=0.6862\n",
      "[Worker 1] √âpoca 10/15 P√©rdida=0.6845                               (0 + 2) / 2]\n",
      "[Worker 1] √âpoca 11/15 P√©rdida=0.6834\n",
      "[Worker 1] √âpoca 12/15 P√©rdida=0.6823\n",
      "[Worker 1] √âpoca 13/15 P√©rdida=0.6804\n",
      "[Worker 1] √âpoca 14/15 P√©rdida=0.6781\n",
      "[Worker 1] √âpoca 15/15 P√©rdida=0.6764\n",
      "2025-11-26 04:01:22,512 WARN  [main] impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(772)) - I/O error constructing remote block reader for block BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:01:22,513 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(667)) - Failed to connect to /100.106.115.1:9866 for file /data/brain_balanced/shard_bal_0006.pt for block BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039, add to deadNodes and continue.\n",
      "org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/100.106.115.1:9866]\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:617)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3076)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:715)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:645)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:157)\n",
      "2025-11-26 04:01:22,637 INFO  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(649)) - Successfully connected to /100.98.144.10:9866 for BP-736167345-127.0.1.1-1763348595625:blk_1073746863_6039\n",
      "/tmp/ipykernel_490470/2673306203.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 0] √âpoca 1/15 P√©rdida=0.6961\n",
      "[Worker 0] √âpoca 2/15 P√©rdida=0.6934\n",
      "[Worker 0] √âpoca 3/15 P√©rdida=0.6913\n",
      "[Worker 0] √âpoca 4/15 P√©rdida=0.6892\n",
      "[Worker 0] √âpoca 5/15 P√©rdida=0.6865\n",
      "[Worker 0] √âpoca 6/15 P√©rdida=0.6846\n",
      "[Worker 0] √âpoca 7/15 P√©rdida=0.6814\n",
      "[Worker 0] √âpoca 8/15 P√©rdida=0.6799                                (0 + 2) / 2]\n",
      "[Worker 0] √âpoca 9/15 P√©rdida=0.6762\n",
      "[Worker 0] √âpoca 10/15 P√©rdida=0.6735\n",
      "[Worker 0] √âpoca 11/15 P√©rdida=0.6699\n",
      "[Worker 0] √âpoca 12/15 P√©rdida=0.6670\n",
      "[Worker 0] √âpoca 13/15 P√©rdida=0.6637\n",
      "[Worker 0] √âpoca 14/15 P√©rdida=0.6592\n",
      "[Worker 0] √âpoca 15/15 P√©rdida=0.6546\n",
      "Finished distributed training with 2 executor processes                         \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados Exp 9:\n",
      "  Loss:      0.6567\n",
      "  Accuracy:  0.7030\n",
      "  Precision: 0.6473\n",
      "  Recall:    0.8920\n",
      "  F1:        0.7502\n",
      "üèÉ View run exp_9_sgd at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/864af8bd2f3741e9b085b93abdba4fab\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      " ‚úì Registrado en MLflow\n",
      "\n",
      "‚úì ENTRENAMIENTO COMPLETO\n",
      "Mejor F1 = 0.9609375\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1\n",
    "best_state_dict = None\n",
    "best_config = None\n",
    "\n",
    "for idx, cfg in enumerate(experiments, 1):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\" EXPERIMENTO {idx}/{len(experiments)}\")\n",
    "    print(f\" Config: {cfg}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ENTRENAMIENTO\n",
    "    model_bytes = TorchDistributor(\n",
    "        num_processes=2,\n",
    "        local_mode=False,\n",
    "        use_gpu=True\n",
    "    ).run(\n",
    "        train_fn,\n",
    "        cfg[\"epochs\"],\n",
    "        cfg[\"lr\"],\n",
    "        cfg[\"batch_size\"],\n",
    "        cfg[\"optimizer\"]\n",
    "    )\n",
    "\n",
    "    # RECONSTRUIR MODELO CORRECTO\n",
    "    state_dict = torch.load(io.BytesIO(model_bytes), map_location=\"cpu\")\n",
    "    model = DeepResNet18()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # PREPARAR TEST\n",
    "    X, Y = [], []\n",
    "    for p in test_shards:\n",
    "        shard = load_pt_with_spark(p)\n",
    "        for i in range(len(shard[\"labels\"])):\n",
    "            img = Image.fromarray(shard[\"images\"][i].numpy())\n",
    "            X.append(tf(img).unsqueeze(0))\n",
    "            Y.append(int(shard[\"labels\"][i]))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, y in zip(X, Y):\n",
    "            out = model(img)\n",
    "            loss = criterion(out, torch.tensor([y]))\n",
    "            total_loss += loss.item()\n",
    "            preds.append(out.argmax(1).item())\n",
    "\n",
    "    avg_loss = total_loss / len(Y)\n",
    "    acc = accuracy_score(Y, preds)\n",
    "    prec = precision_score(Y, preds)\n",
    "    rec = recall_score(Y, preds)\n",
    "    f1 = f1_score(Y, preds)\n",
    "\n",
    "    print(f\" Resultados Exp {idx}:\")\n",
    "    print(f\"  Loss:      {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1:        {f1:.4f}\")\n",
    "\n",
    "    # MLflow aislado\n",
    "    try:\n",
    "        with mlflow.start_run(run_name=f\"exp_{idx}_{cfg['optimizer']}\"):\n",
    "            mlflow.log_params(cfg)\n",
    "            mlflow.log_metric(\"test_loss\", avg_loss)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            mlflow.log_metric(\"precision\", prec)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "        print(\" ‚úì Registrado en MLflow\")\n",
    "    except:\n",
    "        print(\" ‚ö† MLflow fall√≥ pero el entrenamiento contin√∫a\")\n",
    "\n",
    "    # MEJOR MODELO\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_state_dict = state_dict\n",
    "        best_config = cfg.copy()\n",
    "        print(f\" ‚òÖ Nuevo mejor modelo | F1={best_f1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úì ENTRENAMIENTO COMPLETO\")\n",
    "print(\"Mejor F1 =\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf498f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " RECONSTRUYENDO EL MEJOR MODELO \n",
      "======================================================================\n",
      "üèÉ View run best_model at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1/runs/e165bb5b82e54095bd38ec12b3efbadc\n",
      "üß™ View experiment at: https://dagshub.com/picantitoDev/percepcion-proyecto.mlflow/#/experiments/1\n",
      "‚úì MODELO SUBIDO A MLFLOW\n",
      "run_id generado: e165bb5b82e54095bd38ec12b3efbadc\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" RECONSTRUYENDO EL MEJOR MODELO \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_model = DeepResNet18()\n",
    "best_model.load_state_dict(best_state_dict)\n",
    "\n",
    "if os.path.exists(\"best_model\"):\n",
    "    shutil.rmtree(\"best_model\")\n",
    "\n",
    "mlflow.pytorch.save_model(best_model, \"best_model\")\n",
    "\n",
    "# Guardar aqu√≠ el run_id ANTES de cerrar el run\n",
    "run_id = None\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"best_model\") as run:\n",
    "        run_id = run.info.run_id      # <<< AQUI ES LA CLAVE\n",
    "        mlflow.log_params(best_config)\n",
    "        mlflow.log_metric(\"best_f1\", best_f1)\n",
    "        mlflow.log_artifacts(\"best_model\")\n",
    "    print(\"‚úì MODELO SUBIDO A MLFLOW\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö† Fall√≥ MLflow pero modelo local generado:\", e)\n",
    "\n",
    "print(\"run_id generado:\", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd68031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 'ResnetPercepcion' ya existe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 04:18:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ResnetPercepcion, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modelo registrado!\n",
      "Nombre: ResnetPercepcion\n",
      "Versi√≥n: 1\n"
     ]
    }
   ],
   "source": [
    "assert run_id is not None\n",
    "\n",
    "model_uri = f\"runs:/{run_id}/best_model\"\n",
    "model_name = \"ResnetPercepcion\"\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Crear el modelo si no existe\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "    print(f\"Modelo creado: {model_name}\")\n",
    "except:\n",
    "    print(f\"Modelo '{model_name}' ya existe\")\n",
    "\n",
    "# Crear versi√≥n\n",
    "version = client.create_model_version(\n",
    "    name=model_name,\n",
    "    source=model_uri,\n",
    "    run_id=run_id\n",
    ")\n",
    "\n",
    "print(\"‚úì Modelo registrado!\")\n",
    "print(\"Nombre:\", model_name)\n",
    "print(\"Versi√≥n:\", version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4aa7dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490470/3841925311.py:4: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo promovido a producci√≥n\n"
     ]
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"ResnetPercepcion\",\n",
    "    version=1,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True\n",
    ")\n",
    "\n",
    "print(\"Modelo promovido a producci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175de9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

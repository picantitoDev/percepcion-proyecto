{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760a8efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://100.108.67.1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BrainTumor-ResNet18-Distributed-IPYNB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x765379085c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "def train_fn():\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    from torchvision import transforms, datasets, models\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    import io, os\n",
    "\n",
    "    print(\"=== DISTRIBUTED RESNET18 TRAINING (AMP + SHARDING) ===\")\n",
    "\n",
    "    # Distributed metadata\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    print(f\"[Worker {rank}] World size: {world_size}\")\n",
    "\n",
    "    # Dataset path (NFS)\n",
    "    dataset_path = \"/mnt/spark_data/DATASET-RUIDO\"\n",
    "    print(f\"[Worker {rank}] Dataset path: {dataset_path}\")\n",
    "\n",
    "    # Image transforms\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Dataset on NFS\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=train_tf)\n",
    "\n",
    "    # Distributed sharding\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"[Worker {rank}] Total images loaded: {len(dataset)}\")\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Worker {rank}] Training on: {device}\")\n",
    "\n",
    "    # Load ResNet18 pretrained\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Loss, optimizer, AMP\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    EPOCHS = 3\n",
    "    print(f\"[Worker {rank}] Starting training for {EPOCHS} epochs\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        sampler.set_epoch(epoch)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"[Worker {rank}] Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"[Worker {rank}] Training finished!\")\n",
    "\n",
    "    # Return model only from worker 0\n",
    "    if rank == 0:\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(model.state_dict(), buffer)\n",
    "        buffer.seek(0)\n",
    "        return buffer.getvalue()\n",
    "\n",
    "    return None\n",
    "\n",
    "# SPARK CONFIG\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BrainTumor-ResNet18-Distributed-IPYNB\")\n",
    "    .master(\"spark://100.108.67.1:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "    .config(\"spark.executor.resource.gpu.discoveryScript\", \"/usr/local/bin/get-gpus.sh\")\n",
    "    .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "    .config(\"spark.executorEnv.NCCL_SOCKET_IFNAME\", \"tailscale0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e878939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching distributed training with AMP + SHARDING + 2 GPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TorchDistributor:Started distributed training with 2 executor processes\n",
      "=== DISTRIBUTED RESNET18 TRAINING (AMP + SHARDING) ===              (0 + 2) / 2]\n",
      "[Worker 0] World size: 2\n",
      "[Worker 0] Dataset path: /mnt/spark_data/DATASET-RUIDO\n",
      "=== DISTRIBUTED RESNET18 TRAINING (AMP + SHARDING) ===\n",
      "[Worker 1] World size: 2\n",
      "[Worker 1] Dataset path: /mnt/spark_data/DATASET-RUIDO\n",
      "[Worker 1] Total images loaded: 5000                                (0 + 2) / 2]\n",
      "[Worker 1] Training on: cuda\n",
      "/tmp/ipykernel_637271/1158845358.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "[Worker 1] Starting training for 3 epochs\n",
      "/tmp/ipykernel_637271/1158845358.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] Epoch 1/3 - Loss: 13.2996\n",
      "[Worker 0] Total images loaded: 5000                                (0 + 2) / 2]\n",
      "[Worker 0] Training on: cuda\n",
      "/tmp/ipykernel_637271/1158845358.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "[Worker 0] Starting training for 3 epochs\n",
      "/tmp/ipykernel_637271/1158845358.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "[Worker 1] Epoch 2/3 - Loss: 2.9955\n",
      "[Worker 1] Epoch 3/3 - Loss: 1.2369\n",
      "[Worker 1] Training finished!\n",
      "[Worker 0] Epoch 1/3 - Loss: 14.8137                                (0 + 2) / 2]\n",
      "[Worker 0] Epoch 2/3 - Loss: 5.9047                                 (0 + 2) / 2]\n",
      "[Worker 0] Epoch 3/3 - Loss: 2.7041                                 (0 + 2) / 2]\n",
      "[Worker 0] Training finished!\n",
      "INFO:TorchDistributor:Finished distributed training with 2 executor processes   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado correctamente en: /home/piero/brain_resnet18.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Launching distributed training with AMP + SHARDING + 2 GPUs...\")\n",
    "\n",
    "model_bytes = TorchDistributor(\n",
    "    num_processes=2,\n",
    "    local_mode=False,\n",
    "    use_gpu=True\n",
    ").run(train_fn)\n",
    "\n",
    "if model_bytes is not None:\n",
    "    out_path = \"/home/piero/brain_resnet18.pt\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(model_bytes)\n",
    "    print(f\"Modelo guardado correctamente en: {out_path}\")\n",
    "else:\n",
    "    print(\"Worker secundario: no devuelve modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b33362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Env (GPU)",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
